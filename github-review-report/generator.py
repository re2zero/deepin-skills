#!/usr/bin/env python3
"""
GitHub Code Review Report Generator (Chinese Format)

Generates Chinese-format Excel reports from GitHub pull requests with code review summaries.

Usage:
    python generator.py --repo linuxdeepin/dde-cooperation --since "2026-01-01" --module-name dde-cooperation
"""

import argparse
import fnmatch
import os
from datetime import datetime, timedelta
import json
import subprocess
import sys
from typing import List, Dict, Optional

try:
    import pandas as pd
except ImportError:
    print("âŒ pandas not installed. Run: pip install pandas openpyxl")
    sys.exit(1)


# Problem Type Categories (15 types)
PROBLEM_TYPES = {
    1: "ä¹¦å†™è§„èŒƒ",
    2: "æ—¥å¿—è§„èŒƒ",
    3: "å¤´æ–‡ä»¶è§„èŒƒ",
    4: "å˜é‡è§„èŒƒ",
    5: "å¸¸é‡è§„èŒƒ",
    6: "å®å®šä¹‰è§„èŒƒ",
    7: "æŒ‡é’ˆè§„èŒƒ",
    8: "ä»£ç å®‰å…¨æ¼æ´",
    9: "ä»£ç å†—ä½™",
    10: "æ³¨é‡Šè§„èŒƒ",
    11: "ç¼–è¯‘è­¦å‘Š",
    12: "å†…å­˜æœªé‡Šæ”¾",
    13: "æäº¤å†…å®¹è§„èŒƒ",
    14: "ä¸ç¬¦åˆéœ€æ±‚",
    15: "å…¶ä»–",
}

# Problem Source Categories (3 types)
PROBLEM_SOURCES = {
    1: "commit log",
    2: "ä»£ç ",
    3: "æ³¨é‡Š",
}

# Severity Classification
SEVERITY_CRITICAL = "ä¸¥é‡"
SEVERITY_GENERAL = "ä¸€èˆ¬"

CRITICAL_TYPES = {8, 12}  # ä»£ç å®‰å…¨æ¼æ´, å†…å­˜æœªé‡Šæ”¾

# Valid AI reviewers (only these will be included)
VALID_AI_REVIEWERS = {"sourcery-ai"}


def get_problem_type_from_suggestion(suggestion: str) -> int:
    """
    Map review suggestion to problem type category.

    Returns problem type number (1-15).
    """
    suggestion_lower = suggestion.lower()

    # Direct keyword mapping
    type_keywords = {
        "å®‰å…¨": 8,
        "å®‰å…¨æ¼æ´": 8,
        "æ¼æ´": 8,
        "å†…å­˜æ³„æ¼": 12,
        "å†…å­˜": 12,
        "é‡Šæ”¾": 12,
        "æ³¨é‡Š": 10,
        "æ—¥å¿—": 2,
        "ç¼–è¯‘": 11,
        "è­¦å‘Š": 11,
        "å¤´æ–‡ä»¶": 3,
        "å˜é‡": 4,
        "å¸¸é‡": 5,
        "å®": 6,
        "å®å®šä¹‰": 6,
        "æŒ‡é’ˆ": 7,
        "å†—ä½™": 9,
        "æäº¤": 13,
        "éœ€æ±‚": 14,
        "ä¸ç¬¦åˆéœ€æ±‚": 14,
        "æ ¼å¼": 1,
        "å‘½å": 1,
        "ä¹¦å†™": 1,
    }

    for keyword, type_num in type_keywords.items():
        if keyword in suggestion_lower:
            return type_num

    return 15  # Default: å…¶ä»–


def get_severity_from_problem_type(problem_type: int) -> str:
    """
    Determine severity based on problem type.

    Critical (ä¸¥é‡): ä»£ç å®‰å…¨æ¼æ´(8), å†…å­˜æœªé‡Šæ”¾(12)
    General (ä¸€èˆ¬): All other types
    """
    if problem_type in CRITICAL_TYPES:
        return SEVERITY_CRITICAL
    return SEVERITY_GENERAL


def is_valid_person_review(review_body: str, review_state: str) -> bool:
    """
    Check if review is a valid person review (not automated).

    Invalid reviews:
    - Body is empty or only contains "approved", "lgtm", "merge", etc.
    - State is COMMENTED (just comment, not a proper review)
    """
    if not review_body or review_body.strip() == "":
        return False

    review_lower = review_body.lower().strip()

    # Check for common automated approval phrases
    invalid_patterns = [
        "approved",
        "lgtm",
        "looks good to me",
        "merge",
        "force merge",
        "/merge",
        "/forcemerge",
        "mergeable",
        "ready to merge",
        "can merge",
    ]

    for pattern in invalid_patterns:
        if pattern in review_lower:
            return False

    # Check if it's too short (likely just an approval)
    if len(review_body) < 10:
        return False

    return True


def parse_time_range(time_expr: str) -> tuple[datetime, datetime]:
    """Parse relative time expressions to absolute date ranges."""
    time_expr = time_expr.lower().strip()
    today = datetime.now()

    if time_expr == "this month" or time_expr == "è¿™ä¸ªæœˆ":
        start_date = today.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        end_date = today
    elif time_expr == "last month" or time_expr == "ä¸Šä¸ªæœˆ":
        first_day_this_month = today.replace(day=1)
        last_month_end = first_day_this_month - timedelta(days=1)
        first_day_last_month = last_month_end.replace(day=1)
        start_date = first_day_last_month
        end_date = last_month_end
    elif time_expr == "last week" or time_expr == "ä¸Šå‘¨":
        MONDAY = 0
        days_since_monday = today.weekday()
        last_monday = today - timedelta(days=days_since_monday + 7)
        last_sunday = last_monday + timedelta(days=6)
        start_date = last_monday.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = last_sunday.replace(hour=23, minute=59, second=59, microsecond=999999)
    elif time_expr == "this week" or time_expr == "æœ¬å‘¨" or time_expr == "è¿™ä¸ªæ˜ŸæœŸ":
        days_since_monday = today.weekday()
        last_monday = today - timedelta(days=days_since_monday)
        start_date = last_monday.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = today
    elif time_expr.startswith("last") and "day" in time_expr:
        days = int(time_expr.split()[1])
        start_date = today - timedelta(days=days)
        end_date = today
    elif "days ago" in time_expr:
        days = int(time_expr.split()[0])
        start_date = today - timedelta(days=days)
        end_date = today
    elif time_expr.startswith("last") and "week" in time_expr:
        weeks = int(time_expr.split()[1])
        days = weeks * 7
        start_date = today - timedelta(days=days)
        end_date = today
    else:
        raise ValueError(f"Unknown time expression: {time_expr}")

    return start_date, end_date


def format_date_only(iso_date: str) -> str:
    """
    Extract date portion only (YYYY-MM-DD) from ISO datetime string.

    Input: 2026-01-22T14:30:00Z
    Output: 2026-01-22
    """
    if not iso_date:
        return ""
    return iso_date.split('T')[0] if 'T' in iso_date else iso_date


def run_gh_command(args: List[str], json_output: bool = True) -> Dict | str:
    """Run gh CLI command and return output."""
    cmd = ["gh"] + args
    if json_output:
        cmd.extend(["--json", "number,title,author,createdAt,mergedAt,mergedBy,baseRefName,url,reviews,comments"])

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )
        if json_output:
            return json.loads(result.stdout)
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"âŒ gh command failed: {e}")
        print(f"stderr: {e.stderr}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ Failed to parse JSON output: {e}")
        print(f"stdout: {result.stdout}")
        sys.exit(1)


def should_include_reviewer(
    reviewer_name: str,
    include_patterns: Optional[List[str]] = None,
    exclude_patterns: Optional[List[str]] = None
) -> bool:
    """Check if reviewer should be included based on patterns."""
    # Check if this is an AI reviewer (only valid AI reviewers allowed)
    if reviewer_name.lower() in VALID_AI_REVIEWERS:
        # Only include valid AI reviewers (sourcery-ai)
        return True

    # All other AI reviewers are excluded by default
    for ai_reviewer in VALID_AI_REVIEWERS:
        if reviewer_name.lower() != ai_reviewer:
            # This is NOT a valid AI reviewer, exclude it
            return False

    # Check explicit include/exclude patterns
    for pattern in (exclude_patterns or []):
        if fnmatch.fnmatch(reviewer_name.lower(), pattern.lower()):
            return False

    if include_patterns:
        for pattern in include_patterns:
            if fnmatch.fnmatch(reviewer_name.lower(), pattern.lower()):
                return True
        return False
    return True


def summarize_for_person(review_body: str) -> str:
    """
    Summarize person review: copy original text directly.

    Args:
        review_body: Original review content from person

    Returns:
        Original review text (no summarization)
    """
    return review_body.strip()


def generate_ai_impact_analysis(problem_description: str) -> str:
    """
    Generate AI impact analysis based on problem description.

    Args:
        problem_description: The problem description from AI review

    Returns:
        20-character Chinese summary of impact, or "æ— " if no impact
    """
    if not problem_description or len(problem_description.strip()) < 10:
        return "æ— "

    # Simple heuristic: check for impact keywords
    impact_keywords = [
        "å½±å“", "éšæ‚£", "é£é™©", "é—®é¢˜", "ä¿®å¤", "æ”¹è¿›", "å»ºè®®", "æ³¨æ„",
        "è­¦å‘Š", "é”™è¯¯", "å¼‚å¸¸", "bug", "ç¼ºé™·", "ä¼˜åŒ–", "é‡æ„"
    ]

    for keyword in impact_keywords:
        if keyword in problem_description:
            # Extract 20 chars summary around impact keyword
            idx = problem_description.find(keyword)
            start = max(0, idx - 5)
            end = min(len(problem_description), idx + 20)
            summary = problem_description[start:end].strip()
            return summary

    # If no clear impact keyword found
    return "æ— "


def summarize_for_ai(review_body: str, max_length: int = 50) -> str:
    """
    Summarize AI review (sourcery-ai) to Chinese, max 50 characters.

    Args:
        review_body: AI review content
        max_length: Maximum length of summary

    Returns:
        Chinese summary of core points (â‰¤50 chars)
    """
    # Remove code blocks and markdown
    lines = []
    in_code_block = False
    for line in review_body.split('\n'):
        if line.strip().startswith('```'):
            in_code_block = not in_code_block
        elif not in_code_block and line.strip():
            lines.append(line.strip())

    text = ' '.join(lines)

    # Extract core points
    # Remove common prefixes
    prefixes_to_remove = ['Note:', 'Warning:', 'Suggestion:', 'å»ºè®®:', 'Fix:', 'Change:']
    for prefix in prefixes_to_remove:
        if text.startswith(prefix):
            text = text[len(prefix):].strip()

    # Split into sentences
    sentences = []
    for sep in ['ã€‚', '.', '\n', ';']:
        parts = text.split(sep)
        sentences.extend([s.strip() for s in parts if s.strip()])

    # Filter meaningful sentences (exclude very short ones)
    meaningful_sentences = [s for s in sentences if len(s) > 10]

    # Take first meaningful sentences up to max_length
    result = ''
    for sentence in meaningful_sentences:
        if len(result) + len(sentence) > max_length:
            break
        result += sentence + 'ã€‚' if result else sentence

    # Trim to max_length
    if len(result) > max_length:
        result = result[:max_length-3] + "..."

    return result.strip()


def extract_review_suggestions(
    pr_data: Dict,
    target_reviewers: Optional[List[str]] = None
) -> List[Dict]:
    """
    Extract valid review suggestions from PR reviews.

    Returns list of valid review dictionaries (one per valid reviewer per PR).
    Filters out:
    - Invalid automated reviews (approved, lgtm, merge, etc.)
    - Reviews without meaningful content
    - Non-valid AI reviewers (except sourcery-ai)

    Multiple valid reviewers per PR = multiple report rows for that PR.
    """
    valid_reviews = []

    for review in pr_data.get('reviews', []):
        author = review.get('author', {}).get('login', '')
        body = review.get('body', '')
        state = review.get('state', '')

        # Skip if no body or invalid automated review
        if not body:
            continue

        if not is_valid_person_review(body, state):
            continue

        # Filter by reviewer
        if not should_include_reviewer(author):
            continue

        problem_type = get_problem_type_from_suggestion(body)

        valid_reviews.append({
            'content': body,
            'problem_type': problem_type,
            'reviewer': author,
            'review_time': review.get('submittedAt', ''),
        })

    return valid_reviews


def generate_review_report(
    repo: str,
    time_expr: str,
    module_name: str,
    reviewer: str = "liuzheng",
    include_patterns: Optional[List[str]] = None,
    exclude_patterns: Optional[List[str]] = None,
    base_branch: Optional[str] = None,
    limit: Optional[int] = None,
    output_file: Optional[str] = None
) -> str:
    """
    Generate Chinese-format Excel review report.

    Returns:
        Path to generated Excel file
    """
    start_date, end_date = parse_time_range(time_expr)
    start_date_str = start_date.strftime("%Y-%m-%d")
    end_date_str = end_date.strftime("%Y-%m-%d")

    print(f"ğŸ“Š ç”Ÿæˆä»£ç èµ°æŸ¥æŠ¥å‘Š")
    print(f"   æ¨¡å—å: {module_name}")
    print(f"   æ—¶é—´èŒƒå›´: {start_date_str} to {end_date_str}")
    print(f"   ä»“åº“: {repo}")
    print(f"   Reviewer: {reviewer}")
    print()

    search_query = f"merged:{start_date_str}..{end_date_str}"
    if base_branch:
        search_query += f" base:{base_branch}"

    print("ğŸ” è·å–PRæ•°æ®...")
    args = [
        "pr", "list",
        "--repo", repo,
        "--state", "merged",
        "--search", search_query,
    ]

    if limit:
        args.extend(["--limit", str(limit)])

    prs = run_gh_command(args, json_output=True)

    if not prs:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´èŒƒå›´ {start_date_str} to {end_date_str} å†…çš„PR")
        print("å»ºè®®:")
        print("  - æ‰©å¤§æ—¶é—´èŒƒå›´ (ä¾‹å¦‚: 'last month' ä»£æ›¿ 'this week')")
        print("  - æ£€æŸ¥ä»“åº“åç§°æ˜¯å¦æ­£ç¡®")
        print("  - ç¡®è®¤PRå·²åˆå¹¶ (ä¸ä»…ä»…æ˜¯opened)")
        sys.exit(0)

    print(f"   æ‰¾åˆ° {len(prs)} ä¸ªPR")
    print()

    print("ğŸ“ å¤„ç†PRå¹¶æå–æœ‰æ•ˆreview...")
    rows = []
    serial_number = 0

    for i, pr in enumerate(prs, 1):
        print(f"   [{i}/{len(prs)}] PR #{pr['number']}: {pr['title'][:40]}...")

        pr_detail = run_gh_command(
            ["pr", "view", str(pr['number']), "--repo", repo],
            json_output=True
        )

        # Extract valid reviews (filters out automated approvals and invalid AI reviewers)
        valid_reviews = extract_review_suggestions(pr_detail)

        if not valid_reviews:
            print(f"      âš ï¸ æ— æœ‰æ•ˆreview")
            continue

        # Create one row per valid reviewer
        for review in valid_reviews:
            serial_number += 1

            problem_type = review['problem_type']
            severity = get_severity_from_problem_type(problem_type)
            reviewer = review['reviewer']

            # Determine summarization method and impact analysis
            is_ai_reviewer = reviewer.lower() in VALID_AI_REVIEWERS

            if is_ai_reviewer:
                # AI review: summarize to 50 chars
                chinese_summary = summarize_for_ai(review['content'], max_length=50)
                impact_analysis = generate_ai_impact_analysis(chinese_summary)
            else:
                # Person review: copy original text
                chinese_summary = summarize_for_person(review['content'])
                impact_analysis = chinese_summary

            created_at = pr.get('createdAt', '')
            merged_at = pr.get('mergedAt', '')
            review_time = review['review_time']

            author = pr.get('author', {}).get('login', '')
            merged_by = pr.get('mergedBy', {}).get('login', '')

            # Format dates to YYYY-MM-DD only (no time)
            created_date_only = format_date_only(created_at)
            merged_date_only = format_date_only(merged_at)
            review_date_only = format_date_only(review_time)

            problem_status = "å…³é—­" if merged_at else "è§£å†³ä¸­"

            rows.append({
                'åºå·': serial_number,
                'åŒ…å': module_name,
                'ä»“åº“åœ°å€': f"https://github.com/{repo}",
                'ä»£ç æäº¤åœ°å€': pr.get('url', ''),
                'é—®é¢˜æ¥æº': PROBLEM_SOURCES.get(3, 'æ³¨é‡Š'),  # Reviews come from comments
                'é—®é¢˜æè¿°': chinese_summary,
                'ä¸¥é‡ç¨‹åº¦': severity,
                'å½±å“åˆ†æ': impact_analysis,
                'é—®é¢˜ç±»å‹': PROBLEM_TYPES.get(problem_type, 'å…¶ä»–'),
                'æå‡ºäºº': reviewer,
                'æå‡ºæ—¶é—´': review_date_only,
                'è§£å†³äºº': merged_by,
                'è®¡åˆ’è§£å†³æ—¶é—´': merged_date_only,
                'å®é™…è§£å†³æ—¶é—´': merged_date_only,
                'æå‡ºäººç¡®è®¤æ˜¯å¦éªŒæ”¶é€šè¿‡': "æ˜¯",
                'é—®é¢˜çŠ¶æ€': problem_status,
            })

    print()

    if not rows:
        print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆreviewè®°å½•")
        sys.exit(0)

    df = pd.DataFrame(rows)

    yyyymmdd = datetime.now().strftime("%Y%m%d")
    output_file = output_file or f"{module_name}-ä»£ç èµ°æŸ¥æŠ¥å‘Š-{yyyymmdd}.xlsx"

    print("ğŸ’¾ ç”ŸæˆExcelæŠ¥å‘Š...")
    df.to_excel(output_file, index=False, engine='openpyxl')

    print()
    print("âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ!")
    print(f"   æ–‡ä»¶: {output_file}")
    print(f"   æ¨¡å—å: {module_name}")
    print(f"   æ—¶é—´èŒƒå›´: {start_date_str} to {end_date_str}")
    print(f"   Reviewè®°å½•æ•°: {len(rows)}")
    print(f"   Revieweræ€»æ•°: {len(set(row['æå‡ºäºº'] for row in rows))}")

    return output_file


def main():
    parser = argparse.ArgumentParser(
        description='ç”ŸæˆGitHub PRä»£ç èµ°æŸ¥æŠ¥å‘Š(ä¸­æ–‡æ ¼å¼)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æœ¬æœˆæŠ¥å‘Š
  python generator.py --repo linuxdeepin/dde-cooperation --since "this month" --module-name dde-cooperation

  # ä¸Šå‘¨æŠ¥å‘Š
  python generator.py --repo linuxdeepin/dde-cooperation --since "last week" --module-name dde-cooperation

  # æŒ‡å®šreviewerè¿‡æ»¤
  python generator.py --repo linuxdeepin/dde-cooperation --since "last 15 days" --module-name dde-cooperation --reviewer liuzheng --include "sourcery-*" --exclude "*-bot"
        """
    )

    parser.add_argument(
        '--repo',
        required=True,
        help='GitHubä»“åº“ (ä¾‹å¦‚: linuxdeepin/dde-cooperation)'
    )

    parser.add_argument(
        '--since',
        default='this month',
        help='æ—¶é—´èŒƒå›´è¡¨è¾¾å¼ (é»˜è®¤: this month). é€‰é¡¹: "this month", "last month", "last week", "this week", "15 days ago", "2026-01-01..2026-01-31"'
    )

    parser.add_argument(
        '--module-name',
        required=True,
        help='æ¨¡å—å(é¡¹ç›®å),ç”¨äºç”Ÿæˆæ–‡ä»¶åå’ŒåŒ…ååˆ—'
    )

    parser.add_argument(
        '--reviewer',
        default='liuzheng',
        help='Reviewerç”¨æˆ·å (é»˜è®¤: liuzheng)'
    )

    parser.add_argument(
        '--include',
        action='append',
        help='åŒ…å«revieweræ¨¡å¼ (fnmatché£æ ¼, å¯å¤šæ¬¡ä½¿ç”¨)'
    )

    parser.add_argument(
        '--exclude',
        action='append',
        help='æ’é™¤revieweræ¨¡å¼ (fnmatché£æ ¼, å¯å¤šæ¬¡ä½¿ç”¨)'
    )

    parser.add_argument(
        '--base',
        help='è¿‡æ»¤åŸºåˆ†æ”¯ (ä¾‹å¦‚: master, main)'
    )

    parser.add_argument(
        '--limit',
        type=int,
        help='é™åˆ¶PRå¤„ç†æ•°é‡'
    )

    parser.add_argument(
        '--output',
        help='è¾“å‡ºExcelæ–‡ä»¶å'
    )

    args = parser.parse_args()

    generate_review_report(
        repo=args.repo,
        time_expr=args.since,
        module_name=args.module_name,
        reviewer=args.reviewer,
        include_patterns=args.include,
        exclude_patterns=args.exclude,
        base_branch=args.base,
        limit=args.limit,
        output_file=args.output
    )


if __name__ == '__main__':
    main()
